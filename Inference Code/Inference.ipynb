{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0735f9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AdamW\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "from peft import PeftConfig, PeftModel\n",
    "peft_config = PeftConfig.from_pretrained(\"학습시켰던 모델 경로\")\n",
    "base_model_path = peft_config.base_model_name_or_path\n",
    "print(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Your checkpoint path\n",
    "checkpoint_path = \"/\"\n",
    "\n",
    "# Load model and tokenizer from checkpoint_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "transformers_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "peft_model = PeftModel.from_pretrained(transformers_model, checkpoint_path)\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eab737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200자 생성 inference\n",
    "output = peft_model.generate(input_ids=input[\"input_ids\"], max_length=200)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data load for inference\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# test data 질문 끝에 있는 불필요한 따옴표 없애기\n",
    "test_data['질문'] = test_data['질문'].str.strip('\"')\n",
    "\n",
    "# 문장 분리 및 새 칼럼 생성\n",
    "def split_question(row):\n",
    "    # 문장 구분자로 분리\n",
    "    parts = pd.Series(row['질문'].split('. ')).str.strip()  # 마침표 기준 분리\n",
    "    if len(parts) < 2:\n",
    "        parts = pd.Series(row['질문'].split('? ')).str.strip()  # 물음표 기준 분리\n",
    "\n",
    "    # 분리된 문장을 새로운 칼럼에 할당\n",
    "    if len(parts) >= 2:\n",
    "        return pd.Series({'문장1': parts[0], '문장2': parts[1]})\n",
    "    else:\n",
    "        return pd.Series({'문장1': parts[0], '문장2': None})\n",
    "\n",
    "# 적용\n",
    "new_columns = test_data.apply(split_question, axis=1)\n",
    "df = pd.concat([test_data, new_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f946b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_1 = df[df['문장2'].isnull()] # 질문 1개\n",
    "df_question_2 = df[df['문장2'].notnull()] # 질문 2개\n",
    "\n",
    "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "question_1_pred = []\n",
    "\n",
    "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "for test_question in tqdm(df_question_1['문장1']):\n",
    "    prompt = f\"주어진 질문에 대한 답을 출력, 질문: {test_question},답: \"\n",
    "    # 입력 질문을 토큰화하고 모델 입력 형태로 변환\n",
    "    input_seq = tokenizer(prompt,return_tensors='pt').to(device)\n",
    "\n",
    "    # 답변 생성\n",
    "    output = peft_model.generate(\n",
    "        input_ids = input_seq[\"input_ids\"],\n",
    "        max_length=250\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트(답변) 저장\n",
    "    for generated_sequence in output:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
    "        answer_start = full_text.find(\"답: \") + len(\"답: \")\n",
    "        answer_only = full_text[answer_start:].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        question_1_pred.append(answer_only)\n",
    "\n",
    "df_question_1['Answer'] = question_1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c790e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문을 2개로 쪼갠 경우\n",
    "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "question_2_pred_1 = []\n",
    "\n",
    "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "for test_question in tqdm(df_question_2['문장1']):\n",
    "    prompt = f\"주어진 질문에 대한 답을 출력, 질문: {test_question},답: \"\n",
    "    # 입력 질문을 토큰화하고 모델 입력 형태로 변환\n",
    "    input_seq = tokenizer(prompt,return_tensors='pt').to(device)\n",
    "\n",
    "    # 답변 생성\n",
    "    output = peft_model.generate(\n",
    "        input_ids = input_seq[\"input_ids\"],\n",
    "        max_length=150\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트(답변) 저장\n",
    "    for generated_sequence in output:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
    "        answer_start = full_text.find(\"답: \") + len(\"답: \")\n",
    "        answer_only = full_text[answer_start:].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        question_2_pred_1.append(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ddfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "question_2_pred_2 = []\n",
    "\n",
    "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "for test_question in tqdm(df_question_2['문장2']):\n",
    "    prompt = f\"주어진 질문에 대한 답을 출력, 질문: {test_question},답: \"\n",
    "    # 입력 질문을 토큰화하고 모델 입력 형태로 변환\n",
    "    input_seq = tokenizer(prompt,return_tensors='pt').to(device)\n",
    "\n",
    "    # 답변 생성\n",
    "    output = peft_model.generate(\n",
    "        input_ids = input_seq[\"input_ids\"],\n",
    "        max_length=150\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트(답변) 저장\n",
    "    for generated_sequence in output:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
    "        answer_start = full_text.find(\"답: \") + len(\"답: \")\n",
    "        answer_only = full_text[answer_start:].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        question_2_pred_2.append(answer_only)\n",
    "\n",
    "df_question_2['Answer1'] = question_2_pred_1\n",
    "df_question_2['Answer2'] = question_2_pred_2\n",
    "df_question_2[\"Answer\"] = df_question_2[\"Answer1\"] + df_question_2[\"Answer2\"]\n",
    "df_question_1 = df_question_1[['id', 'Answer']]\n",
    "df_question_2 = df_question_2[['id', 'Answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data와 df_question_1 병합\n",
    "merged_1 = pd.merge(test_data, df_question_1, on='id', how='left', suffixes=('', '_q1'))\n",
    "# test_data와 df_question_2 병합\n",
    "merged_2 = pd.merge(test_data, df_question_2, on='id', how='left', suffixes=('', '_q2'))\n",
    "\n",
    "# \"Answer\" 칼럼 채우기\n",
    "# df_question_1의 \"Answer\" 값이 있는 경우 사용, 그렇지 않으면 df_question_2의 값을 사용\n",
    "test_data['Answer'] = merged_1['Answer'].combine_first(merged_2['Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data embedding\n",
    "!pip install sentence_transformers\n",
    "\n",
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "preds_3 = test_data[\"Answer\"]\n",
    "pred_embeddings = model.encode(preds_3)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49144280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "QUESTION = \"도배지에 곰팡이가 생겼을 때 높은 습도가 원인인 경우, 습기 관리는 어떻게 해야 할까요?\"\n",
    "prompt = f\"주어진 질문에 대한 답을 출력, 질문: {QUESTION},답: \"\n",
    "input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = peft_model.generate(\n",
    "    input_ids=input[\"input_ids\"],\n",
    "    max_length=250,\n",
    "    top_k=3,\n",
    "    top_p=0.8,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.3,\n",
    "    num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 제출은 해야되니까~\n",
    "submit = pd.read_csv('sample_submission.csv')\n",
    "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
    "submit.iloc[:,1:] = pred_embeddings\n",
    "submit.to_csv('submit_0305.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
