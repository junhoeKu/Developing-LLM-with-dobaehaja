{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85636527-d0a6-4d46-9481-8f9f5d9c657b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146d3550-a0d9-4602-9c09-222953286b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9900567-97b6-45ee-a00b-813035d9eb26",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cadc43a-1acf-461e-b19b-44314861a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "644it [00:02, 257.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>')\n",
    "\n",
    "# 데이터 포맷팅 및 토크나이징\n",
    "formatted_data = []\n",
    "for _, row in tqdm(data.iterrows()):\n",
    "    for q_col in ['질문_1', '질문_2']:\n",
    "        for a_col in ['category', '답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
    "            # 질문과 답변 쌍을 </s> token으로 연결\n",
    "            input_text = row[q_col] + tokenizer.eos_token + row[a_col]\n",
    "            input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            formatted_data.append(input_ids)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105cb04-bc9c-4bb5-8230-cbd84f34d4fb",
   "metadata": {},
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf4940-6155-43ac-9315-2a17e56a06b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31a4c57bba848969bf8033181ba1ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 - Avg Loss: 2.8083:  73%|██████████████████████████████████▉             | 4693/6440 [2:30:10<55:54,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.to(device) # 모델을 GPU단으로 이동\n",
    "\n",
    "# 모델 학습 하이퍼파라미터(Hyperparameter) 세팅\n",
    "# 실제 필요에 따라 조정하세요.\n",
    "CFG = {\n",
    "    'LR' : 2e-5, # Learning Rate\n",
    "    'EPOCHS' : 10, # 학습 Epoch\n",
    "}\n",
    "\n",
    "# 모델 학습 설정\n",
    "optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "model.train()\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(formatted_data), total=len(formatted_data))\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        # 데이터를 GPU단으로 이동\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 진행률 표시줄에 평균 손실 업데이트\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} - Avg Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "    # 에폭의 평균 손실을 출력\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, Average Loss: {total_loss / len(formatted_data)}\")\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./hansoldeco-kogpt2\")\n",
    "tokenizer.save_pretrained(\"./hansoldeco-kogpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf12d4-544a-4f4c-b5ab-5e7bce2a42cb",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 Fine-tuned 모델과 토크나이저 불러오기\n",
    "model_dir = \"./hansoldeco-kogpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "model.to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
    "\n",
    "# Inference를 위한 test.csv 파일 로드\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
    "preds = []\n",
    "\n",
    "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
    "for test_question in tqdm(test['질문']):\n",
    "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
    "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # 답변 생성\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=300,\n",
    "        temperature=0.9,\n",
    "        top_k=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    # 생성된 텍스트(답변) 저장\n",
    "    for generated_sequence in output_sequences:\n",
    "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
    "        # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
    "        answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)\n",
    "        answer_only = full_text[answer_start:].strip()\n",
    "        answer_only = answer_only.replace('\\n', ' ')\n",
    "        preds.append(answer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3664fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('지원누나꺼에서답변하나만손봄.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9e969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_3 = test_data[\"Answer\"]\n",
    "pred_embeddings = model.encode(preds_3)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a2dd5-5179-40db-b300-16d58706e18e",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce65b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Obtaining dependency information for sentence_transformers from https://files.pythonhosted.org/packages/06/97/57afa3d05801b6b9305f96a7ce5995e12c1d2ba25ce66747de107816b0b5/sentence_transformers-2.3.1-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/cc/07/d6951e3b4079df819d76353302fc3e76835252e7e0b6366f96a03d87ea5f/sentencepiece-0.1.99-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\82109\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\82109\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\82109\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\82109\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\82109\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\82109\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\82109\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\82109\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
      "   ---------------------------------------- 0.0/132.8 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/132.8 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 122.9/132.8 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 132.8/132.8 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "   ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "   ---------------- ---------------------- 419.8/977.5 kB 13.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 849.9/977.5 kB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 977.5/977.5 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.3.1 sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced0f6d3-8edf-4ccd-8901-9782007a8621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "preds_3 = test_data[\"Answer\"]\n",
    "pred_embeddings = model.encode(preds_3)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1069adf-e9c0-46d9-bbee-9f7b4a9dc091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vec_0</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_502</th>\n",
       "      <th>vec_503</th>\n",
       "      <th>vec_504</th>\n",
       "      <th>vec_505</th>\n",
       "      <th>vec_506</th>\n",
       "      <th>vec_507</th>\n",
       "      <th>vec_508</th>\n",
       "      <th>vec_509</th>\n",
       "      <th>vec_510</th>\n",
       "      <th>vec_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>0.045047</td>\n",
       "      <td>-0.033445</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>-0.036463</td>\n",
       "      <td>0.055597</td>\n",
       "      <td>0.045907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014158</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.023483</td>\n",
       "      <td>-0.006140</td>\n",
       "      <td>0.018503</td>\n",
       "      <td>0.016631</td>\n",
       "      <td>-0.005467</td>\n",
       "      <td>0.022610</td>\n",
       "      <td>0.053644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>-0.025969</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>0.021325</td>\n",
       "      <td>-0.006204</td>\n",
       "      <td>0.099643</td>\n",
       "      <td>-0.052744</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>-0.068605</td>\n",
       "      <td>-0.051410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019498</td>\n",
       "      <td>-0.047119</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>-0.053973</td>\n",
       "      <td>-0.016779</td>\n",
       "      <td>0.025215</td>\n",
       "      <td>-0.023197</td>\n",
       "      <td>-0.040746</td>\n",
       "      <td>-0.013399</td>\n",
       "      <td>0.021817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>-0.007217</td>\n",
       "      <td>0.019709</td>\n",
       "      <td>-0.022726</td>\n",
       "      <td>-0.009670</td>\n",
       "      <td>0.131308</td>\n",
       "      <td>-0.036227</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>-0.039610</td>\n",
       "      <td>0.066471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025251</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.085899</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>0.020787</td>\n",
       "      <td>-0.004494</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.059203</td>\n",
       "      <td>0.024421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>-0.006465</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>-0.026738</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.052408</td>\n",
       "      <td>-0.051949</td>\n",
       "      <td>-0.080758</td>\n",
       "      <td>-0.015945</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039307</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.026957</td>\n",
       "      <td>-0.043071</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>-0.046710</td>\n",
       "      <td>-0.029743</td>\n",
       "      <td>-0.025439</td>\n",
       "      <td>0.089134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>0.010277</td>\n",
       "      <td>-0.007995</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-0.004290</td>\n",
       "      <td>0.100152</td>\n",
       "      <td>-0.040631</td>\n",
       "      <td>0.053409</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>-0.010998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015132</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>-0.014686</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>-0.017766</td>\n",
       "      <td>0.047517</td>\n",
       "      <td>-0.018717</td>\n",
       "      <td>0.058538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     vec_0     vec_1     vec_2     vec_3     vec_4     vec_5  \\\n",
       "0  TEST_000  0.015054  0.032182  0.045047 -0.033445  0.074882 -0.000633   \n",
       "1  TEST_001 -0.025969  0.028366  0.021325 -0.006204  0.099643 -0.052744   \n",
       "2  TEST_002 -0.007217  0.019709 -0.022726 -0.009670  0.131308 -0.036227   \n",
       "3  TEST_003 -0.006465  0.034693 -0.026738  0.019800  0.052408 -0.051949   \n",
       "4  TEST_004  0.010277 -0.007995 -0.003341 -0.004290  0.100152 -0.040631   \n",
       "\n",
       "      vec_6     vec_7     vec_8  ...   vec_502   vec_503   vec_504   vec_505  \\\n",
       "0 -0.036463  0.055597  0.045907  ... -0.014158  0.015436 -0.016378 -0.023483   \n",
       "1  0.016071 -0.068605 -0.051410  ... -0.019498 -0.047119  0.020995 -0.053973   \n",
       "2  0.007529 -0.039610  0.066471  ... -0.025251  0.000736  0.085899 -0.000033   \n",
       "3 -0.080758 -0.015945  0.000110  ... -0.039307 -0.002394  0.026957 -0.043071   \n",
       "4  0.053409  0.024780 -0.010998  ... -0.015132  0.008700  0.014739  0.009830   \n",
       "\n",
       "    vec_506   vec_507   vec_508   vec_509   vec_510   vec_511  \n",
       "0 -0.006140  0.018503  0.016631 -0.005467  0.022610  0.053644  \n",
       "1 -0.016779  0.025215 -0.023197 -0.040746 -0.013399  0.021817  \n",
       "2 -0.017271  0.020787 -0.004494 -0.013021 -0.059203  0.024421  \n",
       "3  0.004968  0.053339 -0.046710 -0.029743 -0.025439  0.089134  \n",
       "4 -0.014686  0.023440 -0.017766  0.047517 -0.018717  0.058538  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
    "submit.iloc[:,1:] = pred_embeddings\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d22c4259-c43e-4e2f-9456-d15f249370d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리더보드 제출을 위한 csv파일 생성\n",
    "submit.to_csv('./submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371ca4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seongkeun",
   "language": "python",
   "name": "seongkeun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
